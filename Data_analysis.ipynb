{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2004.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark, sc = _init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: string, CRSDepTime: int, ArrTime: string, CRSArrTime: int, UniqueCarrier: string, FlightNum: int, TailNum: string, ActualElapsedTime: string, CRSElapsedTime: int, AirTime: string, ArrDelay: string, DepDelay: string, Origin: string, Dest: string, Distance: int, TaxiIn: int, TaxiOut: int, Cancelled: int, CancellationCode: string, Diverted: int, CarrierDelay: int, WeatherDelay: int, NASDelay: int, SecurityDelay: int, LateAircraftDelay: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load(filename, \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true',\n",
    "                      delimiter=',',\n",
    "                      inferSchema='true')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing as is stated in the task along with the 'Year'\n",
    "col_to_drop = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted', \n",
    "               'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'Year']\n",
    "df = df.drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|    0|         0|        0|      0|         0|         0|            0|        0|    127|             0|       0|       0|     0|   0|       0|      0|        0|         7001506|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"CancelationCode\" has too much \"null\" (98% of the data) we will remove it too. Others have no missing values except for \"TailNum\", that has only 127 values left.  \n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletion of the \"CancelationCode\" and droping rows that contain \"TailNum\", \"UniqueCarrier\" \n",
    "# is represented by several values so we will explore it later\n",
    "df = df.drop('CancellationCode')\n",
    "df = df.drop('TailNum')\n",
    "## df = df.filter(df.TailNum.isNotNull() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"ArrDelay\" and \"DepDelay\" have string type. We cast them to Integer\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DepDelay\", df[\"DepDelay\"].cast(IntegerType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 7129270,\n",
      "After: 6987729,\n",
      "%:98.0\n"
     ]
    }
   ],
   "source": [
    "old_amount = df.count()\n",
    "df = df.na.drop(\"any\")\n",
    "new_amount = df.count()\n",
    "print( \"Before: \" +str(old_amount) + \",\\nAfter: \" + str(new_amount) + \",\\n%:\"+str(round(new_amount/old_amount, 2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------+---------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------+---------+\n",
      "|    1|        12|        1|    623|       630|       915|           UA|      462|           105|     -14|      -7|   ORD| CLT|     599|     11|        0|\n",
      "|    1|        13|        2|    621|       630|       915|           UA|      462|           105|      -4|      -9|   ORD| CLT|     599|     16|        0|\n",
      "|    1|        14|        3|    633|       630|       915|           UA|      462|           105|       5|       3|   ORD| CLT|     599|     15|        0|\n",
      "|    1|        15|        4|    627|       630|       915|           UA|      462|           105|     -16|      -3|   ORD| CLT|     599|     10|        0|\n",
      "|    1|        16|        5|    635|       630|       915|           UA|      462|           105|       3|       5|   ORD| CLT|     599|     13|        0|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIN REG with only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.select([x[0] for x in df.dtypes if 'int' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+----------+----------+---------+--------------+--------+--------+--------+-------+---------+\n",
      "|Month|DayofMonth|DayOfWeek|CRSDepTime|CRSArrTime|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Distance|TaxiOut|Cancelled|\n",
      "+-----+----------+---------+----------+----------+---------+--------------+--------+--------+--------+-------+---------+\n",
      "|    1|        12|        1|       630|       915|      462|           105|     -14|      -7|     599|     11|        0|\n",
      "|    1|        13|        2|       630|       915|      462|           105|      -4|      -9|     599|     16|        0|\n",
      "|    1|        14|        3|       630|       915|      462|           105|       5|       3|     599|     15|        0|\n",
      "|    1|        15|        4|       630|       915|      462|           105|     -16|      -3|     599|     10|        0|\n",
      "|    1|        16|        5|       630|       915|      462|           105|       3|       5|     599|     13|        0|\n",
      "+-----+----------+---------+----------+----------+---------+--------------+--------+--------+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corr_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Month', 0.008202777323020096),\n",
       " ('DayofMonth', 0.024984890590987054),\n",
       " ('DayOfWeek', -0.017791803836727405),\n",
       " ('CRSDepTime', 0.1267987420931976),\n",
       " ('CRSArrTime', 0.12408222835628847),\n",
       " ('FlightNum', -0.001189494331136067),\n",
       " ('CRSElapsedTime', -0.013223108616871579),\n",
       " ('ArrDelay', 1.0),\n",
       " ('DepDelay', 0.907447881121435),\n",
       " ('Distance', -0.013181189620090234),\n",
       " ('TaxiOut', 0.25188155159775766),\n",
       " ('Cancelled', nan)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I guess it is too pythonic and we nees to change it's PEARSON CORRELATION\n",
    "\n",
    "[(c[0], df.corr(\"ArrDelay\", c[0])) for c in corr_matrix.dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.select(['DepDelay', 'TaxiOut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.fillna(0, subset=(['DepDelay', 'Cancelled', 'ArrDelay']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df).select('features','ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([-7.0, 11.0]), ArrDelay=-14),\n",
       " Row(features=DenseVector([-9.0, 16.0]), ArrDelay=-4),\n",
       " Row(features=DenseVector([3.0, 15.0]), ArrDelay=5),\n",
       " Row(features=DenseVector([-3.0, 10.0]), ArrDelay=-16),\n",
       " Row(features=DenseVector([5.0, 13.0]), ArrDelay=3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = output.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|    features|ArrDelay|\n",
      "+------------+--------+\n",
      "|   (2,[],[])|     -23|\n",
      "|   (2,[],[])|     -23|\n",
      "|   (2,[],[])|     -21|\n",
      "|   (2,[],[])|     -19|\n",
      "|   (2,[],[])|     -18|\n",
      "|   (2,[],[])|     -15|\n",
      "|   (2,[],[])|     -13|\n",
      "|   (2,[],[])|     -12|\n",
      "|   (2,[],[])|     -12|\n",
      "|   (2,[],[])|     -10|\n",
      "|   (2,[],[])|      -3|\n",
      "|   (2,[],[])|      -2|\n",
      "|   (2,[],[])|      -2|\n",
      "|   (2,[],[])|       5|\n",
      "|   (2,[],[])|       7|\n",
      "|[-70.0,68.0]|     -10|\n",
      "|[-60.0,18.0]|      10|\n",
      "|[-50.0,70.0]|       3|\n",
      "|[-39.0,15.0]|     -44|\n",
      "| [-37.0,8.0]|     -38|\n",
      "| [-36.0,5.0]|     -49|\n",
      "|[-36.0,39.0]|     -18|\n",
      "| [-34.0,3.0]|     -33|\n",
      "|[-33.0,36.0]|      -3|\n",
      "| [-32.0,7.0]|     -48|\n",
      "+------------+--------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_reg = LinearRegression(featuresCol = 'features', labelCol='ArrDelay')\n",
    "linear_model = lin_reg.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999208558158891,0.4340318736425392]\n",
      "\n",
      "Intercept: -8.294755647165388\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 12.439089\n",
      "\n",
      "r2: 0.860394\n"
     ]
    }
   ],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------------------+--------------+\n",
      "|        prediction|ArrDelay|          Accuracy|      features|\n",
      "+------------------+--------+------------------+--------------+\n",
      "|-8.294755647165388|     -34| 75.60365986127827|     (2,[],[])|\n",
      "|-8.294755647165388|     -25| 66.82097741133845|     (2,[],[])|\n",
      "|-8.294755647165388|     -12|30.877036273621766|     (2,[],[])|\n",
      "|-8.294755647165388|      -8|3.6844455895673534|     (2,[],[])|\n",
      "|-8.294755647165388|      -2|314.73778235826944|     (2,[],[])|\n",
      "|-8.294755647165388|      -2|314.73778235826944|     (2,[],[])|\n",
      "|-8.294755647165388|       1| 929.4755647165389|     (2,[],[])|\n",
      "|-8.294755647165388|      12|169.12296372637823|     (2,[],[])|\n",
      "|-236.1063931166072|      -7|3272.9484730943886|  [-230.0,5.0]|\n",
      "|-96.77670545154987|       5|2035.5341090309973|[-174.0,197.0]|\n",
      "+------------------+--------+------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "predictions = linear_model.transform(test)\n",
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.862328\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+----+\n",
      "|DepTime|UniqueCarrier|Origin|Dest|\n",
      "+-------+-------------+------+----+\n",
      "|    623|           UA|   ORD| CLT|\n",
      "|    621|           UA|   ORD| CLT|\n",
      "|    633|           UA|   ORD| CLT|\n",
      "|    627|           UA|   ORD| CLT|\n",
      "|    635|           UA|   ORD| CLT|\n",
      "+-------+-------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NON_corr_matrix = df.select([x[0] for x in df.dtypes if x[1] !='int'])\n",
    "NON_corr_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = NON_corr_matrix.select(\"Origin\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Origin='BGM'),\n",
       " Row(Origin='DLG'),\n",
       " Row(Origin='MSY'),\n",
       " Row(Origin='GEG'),\n",
       " Row(Origin='BUR')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.stat import Correlation\n",
    "\n",
    "# data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "#         (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "#         (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "#         (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "# df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# r1 = Correlation.corr(df, \"features\").head()\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "# r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "# print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Approach\n",
    "# from pyspark.ml.stat import Correlation\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # convert to vector column first\n",
    "# vector_col = \"corr_features\"\n",
    "# assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
    "# df_vector = assembler.transform(df).select(vector_col)\n",
    "\n",
    "# # get correlation matrix\n",
    "# matrix = Correlation.corr(df_vector, vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to visualize data, it has to be transformed in Pandas\n",
    "#unfortunately, our dataset is too large, therefore we only have to get a sample\n",
    "# in this case we only get 25% of our data, with no replacement\n",
    "\n",
    "df_Pandas_25 = df.sample(False, 0.25, 42).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will be using Altair for visualization, which accepts only 5000 max observations\n",
    "#from here we can tell what airports have the longest trips\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "alt.Chart(df_Pandas_25.sample(n=5000, random_state=1)).mark_point().encode(\n",
    "    x='Origin',\n",
    "    y='Distance',\n",
    "    color='DayOfWeek',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=df_Pandas_25.Origin,\n",
    "    y=df_Pandas_25.Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
