{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2004.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark, sc = _init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: int, Month: int, DayofMonth: int, DayOfWeek: int, DepTime: string, CRSDepTime: int, ArrTime: string, CRSArrTime: int, UniqueCarrier: string, FlightNum: int, TailNum: string, ActualElapsedTime: string, CRSElapsedTime: int, AirTime: string, ArrDelay: string, DepDelay: string, Origin: string, Dest: string, Distance: int, TaxiIn: int, TaxiOut: int, Cancelled: int, CancellationCode: string, Diverted: int, CarrierDelay: int, WeatherDelay: int, NASDelay: int, SecurityDelay: int, LateAircraftDelay: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load(filename, \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true',\n",
    "                      delimiter=',',\n",
    "                      inferSchema='true')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing as is stated in the task along with the 'Year'\n",
    "col_to_drop = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted', \n",
    "               'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'Year']\n",
    "df = df.drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|    0|         0|        0|      0|         0|         0|            0|        0|    127|             0|       0|       0|     0|   0|       0|      0|        0|         7001506|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"CancelationCode\" has too much \"null\" (98% of the data) we will remove it too. Others have no missing values except for \"TailNum\", that has only 127 values left.  \n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletion of the \"CancelationCode\" and droping rows that contain \"TailNum\", \"UniqueCarrier\" \n",
    "# is represented by several values so we will explore it later\n",
    "df = df.drop('CancellationCode')\n",
    "df = df.drop('TailNum')\n",
    "## df = df.filter(df.TailNum.isNotNull() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"ArrDelay\" and \"DepDelay\" have string type. We cast them to Integer\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DepDelay\", df[\"DepDelay\"].cast(IntegerType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 7129270,\n",
      "After: 6987729,\n",
      "%:98.0\n"
     ]
    }
   ],
   "source": [
    "old_amount = df.count()\n",
    "df = df.na.drop(\"any\")\n",
    "new_amount = df.count()\n",
    "print( \"Before: \" +str(old_amount) + \",\\nAfter: \" + str(new_amount) + \",\\n%:\"+str(round(new_amount/old_amount, 2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIN REG with only numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.select([x[0] for x in df.dtypes if 'int' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I guess it is too pythonic and we nees to change it's PEARSON CORRELATION\n",
    "\n",
    "[(c[0], df.corr(\"ArrDelay\", c[0])) for c in corr_matrix.dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.select(['DepDelay', 'TaxiOut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.fillna(0, subset=(['DepDelay', 'Cancelled', 'ArrDelay']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df).select('features','ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = output.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_reg = LinearRegression(featuresCol = 'features', labelCol='ArrDelay')\n",
    "linear_model = lin_reg.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "predictions = linear_model.transform(test)\n",
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`features`' given input columns: [ArrDelay, CRSArrTime, CRSDepTime, CRSElapsedTime, Cancelled, DayOfWeek, DayofMonth, DepDelay, DepTime, Dest, Distance, FlightNum, Month, Origin, TaxiOut, UniqueCarrier];;\n'Project [ArrDelay#897, 'features, Origin#32, Dest#33, DepDelay#914, TaxiOut#36]\n+- Filter AtLeastNNulls(n, Month#17,DayofMonth#18,DayOfWeek#19,DepTime#20,CRSDepTime#21,CRSArrTime#23,UniqueCarrier#24,FlightNum#25,CRSElapsedTime#28,ArrDelay#897,DepDelay#914,Origin#32,Dest#33,Distance#34,TaxiOut#36,Cancelled#37)\n   +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, ArrDelay#897, cast(DepDelay#31 as int) AS DepDelay#914, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n      +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, cast(ArrDelay#30 as int) AS ArrDelay#897, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n         +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n            +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, TailNum#26, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n               +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, TailNum#26, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37, CancellationCode#38]\n                  +- Relation[Year#16,Month#17,DayofMonth#18,DayOfWeek#19,DepTime#20,CRSDepTime#21,ArrTime#22,CRSArrTime#23,UniqueCarrier#24,FlightNum#25,TailNum#26,ActualElapsedTime#27,CRSElapsedTime#28,AirTime#29,ArrDelay#30,DepDelay#31,Origin#32,Dest#33,Distance#34,TaxiIn#35,TaxiOut#36,Cancelled#37,CancellationCode#38,Diverted#39,... 5 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a0fb53f77523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mselectedCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ArrDelay'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselectedCols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`features`' given input columns: [ArrDelay, CRSArrTime, CRSDepTime, CRSElapsedTime, Cancelled, DayOfWeek, DayofMonth, DepDelay, DepTime, Dest, Distance, FlightNum, Month, Origin, TaxiOut, UniqueCarrier];;\n'Project [ArrDelay#897, 'features, Origin#32, Dest#33, DepDelay#914, TaxiOut#36]\n+- Filter AtLeastNNulls(n, Month#17,DayofMonth#18,DayOfWeek#19,DepTime#20,CRSDepTime#21,CRSArrTime#23,UniqueCarrier#24,FlightNum#25,CRSElapsedTime#28,ArrDelay#897,DepDelay#914,Origin#32,Dest#33,Distance#34,TaxiOut#36,Cancelled#37)\n   +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, ArrDelay#897, cast(DepDelay#31 as int) AS DepDelay#914, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n      +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, cast(ArrDelay#30 as int) AS ArrDelay#897, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n         +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n            +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, TailNum#26, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37]\n               +- Project [Month#17, DayofMonth#18, DayOfWeek#19, DepTime#20, CRSDepTime#21, CRSArrTime#23, UniqueCarrier#24, FlightNum#25, TailNum#26, CRSElapsedTime#28, ArrDelay#30, DepDelay#31, Origin#32, Dest#33, Distance#34, TaxiOut#36, Cancelled#37, CancellationCode#38]\n                  +- Relation[Year#16,Month#17,DayofMonth#18,DayOfWeek#19,DepTime#20,CRSDepTime#21,ArrTime#22,CRSArrTime#23,UniqueCarrier#24,FlightNum#25,TailNum#26,ActualElapsedTime#27,CRSElapsedTime#28,AirTime#29,ArrDelay#30,DepDelay#31,Origin#32,Dest#33,Distance#34,TaxiIn#35,TaxiOut#36,Cancelled#37,CancellationCode#38,Diverted#39,... 5 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "df1 = df.select('Origin', 'Dest','DepDelay', 'TaxiOut')\n",
    "cols = df1.columns\n",
    "\n",
    "categoricalColumns = ['Origin', 'Dest']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "\n",
    "    \n",
    "     \n",
    "label_stringIdx = StringIndexer(inputCol = 'DepDelay', outputCol = 'ArrDelay')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "\n",
    "numericCols = ['DepDelay', 'TaxiOut']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df1)\n",
    "df1 = pipelineModel.transform(df1)\n",
    "selectedCols = ['ArrDelay', 'features'] + cols\n",
    "df1 = df.select(selectedCols)\n",
    "df1.printSchema()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "df = spark.createDataFrame([(0, \"a\", 1), (1, \"b\", 2), (2, \"c\", 3), (3, \"a\", 4), (4, \"a\", 4), (5, \"c\", 3)], [\"id\", \"category1\", \"category2\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category1\", outputCol=\"category1Index\")\n",
    "inputs = [indexer.getOutputCol(), \"category2\"]\n",
    "encoder = OneHotEncoder(inputCols=inputs, outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "pipeline.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_corr_matrix = df.select([x[0] for x in df.dtypes if x[1] !='int'])\n",
    "NON_corr_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = NON_corr_matrix.select(\"Origin\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.stat import Correlation\n",
    "\n",
    "# data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "#         (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "#         (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "#         (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "# df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "# r1 = Correlation.corr(df, \"features\").head()\n",
    "# print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "# r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "# print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Approach\n",
    "# from pyspark.ml.stat import Correlation\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# # convert to vector column first\n",
    "# vector_col = \"corr_features\"\n",
    "# assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
    "# df_vector = assembler.transform(df).select(vector_col)\n",
    "\n",
    "# # get correlation matrix\n",
    "# matrix = Correlation.corr(df_vector, vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix.collect()[0][\"pearson({})\".format(vector_col)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to visualize data, it has to be transformed in Pandas\n",
    "#unfortunately, our dataset is too large, therefore we only have to get a sample\n",
    "# in this case we only get 25% of our data, with no replacement\n",
    "\n",
    "df_Pandas_25 = df.sample(False, 0.25, 42).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will be using Altair for visualization, which accepts only 5000 max observations\n",
    "#from here we can tell what airports have the longest trips\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "alt.Chart(df_Pandas_25.sample(n=5000, random_state=1)).mark_point().encode(\n",
    "    x='Origin',\n",
    "    y='Distance',\n",
    "    color='DayOfWeek',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=df_Pandas_25.Origin,\n",
    "    y=df_Pandas_25.Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
