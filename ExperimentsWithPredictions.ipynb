{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2004.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fc0f46009325>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_init_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fc0f46009325>\u001b[0m in \u001b[0;36m_init_spark\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_init_spark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Project\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def _init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc\n",
    "\n",
    "spark, sc = _init_spark()\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load(filename, \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true',\n",
    "                      delimiter=',',\n",
    "                      inferSchema='true')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted', 'CancellationCode',\n",
    "               'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'Year', 'TailNum', ]\n",
    "df = df.drop(*col_to_drop)\n",
    "\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DepDelay\", df[\"DepDelay\"].cast(IntegerType()))\n",
    "df = df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from  pyspark.sql.functions import abs\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_PCA = df.drop('UniqueCarrier').drop(\"Origin\").drop(\"Dest\").drop(\"DepTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_PCA.columns[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_features  = df.drop(\"ArrDel\")\n",
    "assembler = VectorAssembler(inputCols=for_PCA.columns[:7]+for_PCA.columns[8:], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = assembler.transform(for_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol='features', outputCol='PCAfeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = pca.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca_model.transform(data).select('PCAfeatures', 'ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = assembler.transform(for_PCA).select('features', 'ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = pca_data.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(featuresCol = 'PCAfeatures', labelCol='ArrDelay')\n",
    "linear_model = lin_reg.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(linear_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(linear_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSummary = linear_model.summary\n",
    "print(\"RMSE: %f\" % trainSummary.rootMeanSquaredError)\n",
    "print(\"\\nr2: %f\" % trainSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = linear_model.transform(test)\n",
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"PCAfeatures\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.select(['DepDelay', 'TaxiOut'])\n",
    "\n",
    "gen_assembler = VectorAssembler(\n",
    "    inputCols=features.columns,\n",
    "    outputCol='features')\n",
    "\n",
    "gen_output = gen_assembler.transform(df).select('features','ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train,gen_test = gen_output.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"Identity\", maxIter=10, regParam=0.3, labelCol='ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = glr.fit(gen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + str(gen_model.coefficients))\n",
    "print(\"\\nIntercept: \" + str(gen_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSummary = gen_model.summary\n",
    "trainSummary.pValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gen_model.transform(gen_test)\n",
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree and Random Forest Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.select(['DepDelay', 'TaxiOut', 'ArrDelay'])\n",
    "\n",
    "gen_assembler = VectorAssembler(\n",
    "    inputCols=features.columns[:-1],\n",
    "    outputCol='features')\n",
    "\n",
    "gen_output = gen_assembler.transform(df).select('features','ArrDelay')\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol='features', outputCol='IndexedFeatures').fit(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = gen_output.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(featuresCol=\"IndexedFeatures\", labelCol='ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[featureIndexer, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", 'ArrDelay', \"features\").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "    pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                     labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "    print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "    treeModel = model.stages[1]\n",
    "    # summary only\n",
    "    print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol=\"IndexedFeatures\", labelCol='ArrDelay')\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\", \"ArrDelay\", \"features\").show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "    pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                     labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "    print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "    rfModel = model.stages[1]\n",
    "    print(rfModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machines Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('DepTime').drop('UniqueCarrier').drop('Origin').drop('Dest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_assembler = VectorAssembler(\n",
    "    inputCols=features.columns[:7]+features.columns[8:],\n",
    "    outputCol='features')\n",
    "\n",
    "gen_output = gen_assembler.transform(df).select('features','ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output.select(\"ArrDelay\").show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import FMRegressor\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features.\n",
    "featureScaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(gen_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = gen_output.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a FM model.\n",
    "fm = FMRegressor(featuresCol=\"scaledFeatures\", stepSize=0.001, labelCol='ArrDelay')\n",
    "\n",
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=[featureScaler, fm])\n",
    "\n",
    "# Train model.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", 'ArrDelay', \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "fmModel = model.stages[1]\n",
    "print(\"Factors: \" + str(fmModel.factors))\n",
    "print(\"Linear: \" + str(fmModel.linear))\n",
    "print(\"Intercept: \" + str(fmModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.select(['DepDelay', 'TaxiOut', 'ArrDelay'])\n",
    "\n",
    "gen_assembler = VectorAssembler(\n",
    "    inputCols=features.columns[:-1],\n",
    "    outputCol='features')\n",
    "\n",
    "gen_output = gen_assembler.transform(df).select('features','ArrDelay')\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol='features', outputCol='IndexedFeatures').fit(gen_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = gen_output.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"IndexedFeatures\", labelCol=\"ArrDelay\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"ArrDelay\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"ArrDelay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = False\n",
    "\n",
    "(str(val).lower() == 'true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Stage (Ignore info Above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Bucketizer, Normalizer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import isnan, when, count, col, abs\n",
    "from pyspark.sql import functions as sf \n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "filename = \"2004.csv\"\n",
    "\n",
    "def _init_spark():\n",
    "    spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc\n",
    "\n",
    "spark, sc = _init_spark()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load(filename, \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true',\n",
    "                      delimiter=',',\n",
    "                      inferSchema='true')\n",
    "df.cache()\n",
    "\n",
    "col_to_drop = ['ArrTime', 'ActualElapsedTime', 'AirTime', 'TaxiIn', 'Diverted',\n",
    "               'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', \n",
    "               'Year', 'TailNum', 'CancellationCode' ] # Only those 3 I added up to delay, others \n",
    "                                                       # are delayed as is stated in the task\n",
    "df = df.drop(*col_to_drop)\n",
    "df = df.withColumn(\"ArrDelay\", df[\"ArrDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DepDelay\", df[\"DepDelay\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"CRSDepTime\", df[\"CRSDepTime\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"CRSArrTime\", df[\"CRSArrTime\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DepTime\", df[\"DepTime\"].cast(IntegerType()))\n",
    "\n",
    "#These are new lines to add\n",
    "df = df.withColumn(\"Month\", df[\"Month\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"DayOfWeek\", df[\"DayOfWeek\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"CRSElapsedTime\", df[\"CRSElapsedTime\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Distance\", df[\"Distance\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"TaxiOut\", df[\"TaxiOut\"].cast(IntegerType()))\n",
    "#end here\n",
    "\n",
    "\n",
    "df = df.filter(\"Cancelled == 0\") #select only those flights that happened\n",
    "df = df.drop(\"Cancelled\")\n",
    "df = df.drop(*[\"UniqueCarrier\", \"DayofMonth\", \"FlightNum\"]) #Droping unimportant categorical variables\n",
    "\n",
    "df = df.na.drop(\"any\") # Drop columns with null values +- 99% of dataset remains \n",
    "\n",
    "df = df.withColumn('OrigDest', \n",
    "                    sf.concat(sf.col('Origin'),sf.lit('_'), sf.col('Dest')))\n",
    "df = df.withColumn(\"Speed\", sf.round(col(\"Distance\") / col(\"CRSElapsedTime\"), 2))\n",
    "#df = df.withColumnRenamed(\"ArrDelay\", \"label\")\n",
    "\n",
    "#These are new lines to add\n",
    "df = df.drop(*[\"Origin\", \"Dest\", \"Distance\", \"CRSElapsedTime\"])\n",
    "#end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.5, 0.1], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+----------+----------+--------+--------+-------+--------+-----+\n",
      "|Month|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|ArrDelay|DepDelay|TaxiOut|OrigDest|Speed|\n",
      "+-----+---------+-------+----------+----------+--------+--------+-------+--------+-----+\n",
      "|    1|        1|      2|      2355|       855|       3|       7|     17| HNL_PHX|  8.1|\n",
      "|    1|        1|      3|      2355|       855|      -1|       8|     11| HNL_PHX|  8.1|\n",
      "|    1|        1|      4|      2215|      2335|     101|     109|     15| SFO_ONT| 4.54|\n",
      "|    1|        1|      7|      2000|      2142|     261|     247|     35| ORD_MEM| 4.81|\n",
      "|    1|        1|      7|      2220|      2341|      94|     107|     11| CLT_ATL|  2.8|\n",
      "+-----+---------+-------+----------+----------+--------+--------+-------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)\n",
    "#train = train.limit(1000000)\n",
    "#test = test.limit(250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = ['DepDelay', 'TaxiOut']\n",
    "X2 = ['DepDelay', 'TaxiOut',  'HotDepTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), 500, 1200, 1700, float(\"inf\")]\n",
    "\n",
    "\n",
    "\n",
    "bucketizer = Bucketizer(splitsArray= [splits, splits, splits], \\\n",
    "                        inputCols=[\"CRSDepTime\", \"CRSArrTime\", \"DepTime\"],\\\n",
    "                        outputCols=[\"CatCRSDepTime\", \"CatCRSArrTime\", \"CatDepTime\"])\n",
    "\n",
    "varIdxer = StringIndexer(inputCol=\"OrigDest\", outputCol=\"IndOrigDest\")\n",
    "\n",
    "\n",
    "oneHot = OneHotEncoder(inputCols=['Month', 'DayOfWeek', 'CatCRSDepTime', 'CatCRSArrTime',\\\n",
    "                                  'IndOrigDest', 'CatDepTime'],\n",
    "                       outputCols=['HotMonth', 'HotDayOfWeek', 'HotCRSCatDepTime', 'HotCRSCatArrTime',\\\n",
    "                                   'HotIndOrigDest', 'HotDepTime'])\n",
    "\n",
    "assembler = VectorAssembler(inputCols=X2, outputCol='features')\n",
    "\n",
    "#scaler = StandardScaler(inputCol='feat', outputCol=\"features\",\n",
    "#                        withStd=True, withMean=False)\n",
    "\n",
    "lin_reg = LinearRegression(featuresCol = 'features', labelCol=\"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(*[\"CRSDepTime\", \"CRSArrTime\"])\n",
    "# df = df.drop(\"OrigDest\")\n",
    "# df = df.drop(*['Month', 'DayOfWeek', 'CatDepTime', 'CatCRSDepTime', 'CatCRSArrTime', 'IndOrigDest'])\n",
    "# df = df.drop(*[\"Distance\", \"CRSElapsedTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[bucketizer, varIdxer, oneHot, assembler,  lin_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linParamGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lin_reg.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lin_reg.fitIntercept, [False, True])\\\n",
    "    .addGrid(lin_reg.elasticNetParam, [0.0, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "#.addGrid(lin_reg.maxIter, [1, 3])\n",
    "\n",
    "tvs = CrossValidator(estimator=pipeline,\\\n",
    "                           estimatorParamMaps = linParamGrid,  \n",
    "                           evaluator=RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"rmse\"),\\\n",
    "                           numFolds=3)\n",
    "                           #trainRatio=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(train)\n",
    "#model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)\n",
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "#predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.920564\n",
      "Root Mean Squared Error (RMSE) on test data = 11.1777\n",
      "Mean Absolute Error (MAE) on test data = 8.0773\n"
     ]
    }
   ],
   "source": [
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R Squared (R2) on test data = 0.832352\n",
    "Root Mean Squared Error (RMSE) on test data = 12.9493\n",
    "Mean Absolute Error (MAE) on test data = 8.24118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid + CV\n",
    "R Squared (R2) on test data = 0.89077\n",
    "Root Mean Squared Error (RMSE) on test data = 12.4729\n",
    "Mean Absolute Error (MAE) on test data = 8.31376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "#from pyspark.ml.regression import FMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"ArrDelay\", maxIter=10)\n",
    "#fm = FMRegressor(featuresCol=\"features\", stepSize=0.001, labelCol='ArrDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[bucketizer, varIdxer, oneHot, assembler, fm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeParamGrid = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxDepth, [2, 10])\\\n",
    "    .addGrid(gbt.maxBins, [10, 20])\\\n",
    "    .build()\n",
    "\n",
    "tvs = CrossValidator(estimator=pipeline,\n",
    "                           estimatorParamMaps=TreeParamGrid, #remove if don't want to use ParamGridBuilder\n",
    "                           evaluator=RegressionEvaluator(labelCol=\"ArrDelay\", metricName=\"rmse\"),\n",
    "                           numFolds=3)\n",
    "                     #trainRatio=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+----------+----------+--------------+--------+--------+--------+-------+--------+-----+\n",
      "|Month|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|CRSElapsedTime|ArrDelay|DepDelay|Distance|TaxiOut|OrigDest|Speed|\n",
      "+-----+---------+-------+----------+----------+--------------+--------+--------+--------+-------+--------+-----+\n",
      "|    1|        1|      2|      2355|       855|           360|       3|       7|    2917|     17| HNL_PHX|  8.1|\n",
      "|    1|        1|      3|      2355|       855|           360|      -1|       8|    2917|     11| HNL_PHX|  8.1|\n",
      "|    1|        1|      4|      2215|      2335|            80|     101|     109|     363|     15| SFO_ONT| 4.54|\n",
      "|    1|        1|      7|      2000|      2142|           102|     261|     247|     491|     35| ORD_MEM| 4.81|\n",
      "|    1|        1|      7|      2220|      2341|            81|      94|     107|     227|     11| CLT_ATL|  2.8|\n",
      "+-----+---------+-------+----------+----------+--------------+--------+--------+--------+-------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(train)\n",
    "#model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =((predictions['ArrDelay']-predictions['prediction'])/predictions['ArrDelay'])*100\n",
    "predictions = predictions.withColumn('Accuracy',abs(x))\n",
    "#predictions.select(\"prediction\",\"ArrDelay\",\"Accuracy\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.615811\n",
      "Root Mean Squared Error (RMSE) on test data = 24.5814\n",
      "Mean Absolute Error (MAE) on test data = 13.1251\n"
     ]
    }
   ],
   "source": [
    "pred_evaluator = RegressionEvaluator(predictionCol=\"prediction\",labelCol=\"ArrDelay\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % pred_evaluator.evaluate(predictions))\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='ArrDelay', predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'HotDepTime']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "X.append({ \"name\": \"X1\", \"variables\": ['DepDelay', 'TaxiOut']})\n",
    "X.append({ \"name\": \"X2\", \"variables\": ['DepDelay', 'TaxiOut',  'HotDepTime']})\n",
    "X.append({ \"name\": \"X3\", \"variables\": ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'HotDepTime']})\n",
    "X.append({ \"name\": \"X4\", \"variables\": ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotMonth', 'Speed']})\n",
    "X.append({ \"name\": \"X5\", \"variables\": ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotIndOrigDest', 'Speed']})\n",
    "X.append({ \"name\": \"X6\", \"variables\": ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'Speed', 'HotCRSCatDepTime', 'HotCRSCatArrTime', 'HotDepTime']})\n",
    "\n",
    "X[2]['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = []\n",
    "df.append({'name': 'X1', 'variables': ['DepDelay', 'TaxiOut'],'R2LR': 0.860337, 'maeLR': 7.83117, 'rmseLR': 12.4009, 'R2RF': 0.7113226179229843, 'maeRF': 9.632419894894845, 'rmseRF': 17.870397493687378, 'R2DT': 0.833566, 'maeDT': 7.76935, 'rmseDT': 13.5342, 'R2GBR': 0.6779749591610146, 'maeGBR': 10.291720285041185, 'rmseGBR': 18.93692861799665})\n",
    "df.append({'name': 'X2', 'variables': ['DepDelay', 'TaxiOut', 'HotDepTime'], 'R2LR': 0.860466, 'maeLR': 7.82792, 'rmseLR': 12.3951 , 'R2RF': 0.6618056282411904, 'maeRF': 10.69581629434264, 'rmseRF': 19.317928448876994, 'R2DT': 0.7359741049968364, 'maeDT': 9.276273778053989, 'rmseDT': 17.06870714215145, 'R2GBR': 0.7317420904346483, 'maeGBR': 8.919500016072934, 'rmseGBR': 17.40522038041701})\n",
    "df.append({'name': 'X3', 'variables': ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'HotDepTime'], 'R2LR':  0.867357, 'maeLR': 7.39696, 'rmseLR': 12.114, 'R2RF': 0.6893474138730874, 'maeRF': 10.391762216453797, 'rmseRF': 18.48095278940399, 'R2DT': 0.7429083054863235, 'maeDT': 9.244540689810753, 'rmseDT': 16.812445684525386, 'R2GBR': 0.7340066674151544, 'maeGBR': 8.970977217618819, 'rmseGBR': 17.224534616062524})\n",
    "df.append({'name': 'X4', 'variables': ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotMonth', 'Speed'], 'R2LR': 0.860717, 'maeLR': 7.79462, 'rmseLR': 12.4614, 'R2RF': 0.6473918259213414, 'maeRF': 10.698236247649278, 'rmseRF': 19.77355050663056, 'R2DT': 0.734263104956572, 'maeDT': 9.320667734524053, 'rmseDT': 17.16581381584866, 'R2GBR': 0.7466087502783435, 'maeGBR': 8.799469470636394, 'rmseGBR': 16.83094772729247})\n",
    "df.append({'name': 'X5', 'variables': ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotIndOrigDest', 'Speed'], 'R2LR': 0.87867, 'maeLR': 7.121, 'rmseLR': 11.6102, 'R2RF': 0.6450061986902308, 'maeRF': 10.972519639797552, 'rmseRF': 19.980872071516536, 'R2DT': 0.7374275663556327, 'maeDT': 9.256949155299973, 'rmseDT': 17.184172320005104, 'R2GBR': 0.7402478789139912, 'maeGBR': 8.916626117462856, 'rmseGBR': 17.048795624738972})\n",
    "df.append({'name': 'X6', 'variables': ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'Speed', 'HotCRSCatDepTime', 'HotCRSCatArrTime', 'HotDepTime'], 'R2LR': 0.8775881, 'maeLR': 7.06387, 'rmseLR': 11.7076 , 'R2RF': 0.6932635305087731, 'maeRF': 10.377749609744654, 'rmseRF': 18.532997429411278, 'R2DT': 0.7426814121619514, 'maeDT': 9.235528412342111, 'rmseDT': 16.974560476441024, 'R2GBR': 0.7393962128806398, 'maeGBR': 8.897008426472556, 'rmseGBR': 16.96102270986277})\n",
    "\n",
    "DF = pd.DataFrame(df)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = []\n",
    "df.append({'name': 'X1', 'variables': ['DepDelay', 'TaxiOut'],'R2LR': 0.860337, 'maeLR': 7.83117, 'rmseLR': 12.4009 })\n",
    "df.append({'name': 'X2', 'variables': ['DepDelay', 'TaxiOut', 'HotDepTime'], 'R2LR': 0.860466, 'maeLR': 7.82792, 'rmseLR': 12.3951  })\n",
    "df.append({'name': 'X3', 'variables': ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'HotDepTime'], 'R2LR':  0.867357, 'maeLR': 7.39696, 'rmseLR': 12.114 })\n",
    "df.append({'name': 'X4', 'variables': ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotMonth', 'Speed'], 'R2LR': 0.860717, 'maeLR': 7.79462, 'rmseLR': 12.4614 })\n",
    "df.append({'name': 'X5', 'variables': ['DepDelay', 'TaxiOut', 'HotDayOfWeek', 'HotIndOrigDest', 'Speed'], 'R2LR': 0.87867, 'maeLR': 7.121, 'rmseLR': 11.6102 })\n",
    "df.append({'name': 'X6', 'variables': ['DepDelay', 'TaxiOut', 'HotIndOrigDest', 'Speed', 'HotCRSCatDepTime', 'HotCRSCatArrTime', 'HotDepTime'], 'R2LR': 0.8775881, 'maeLR': 7.06387, 'rmseLR': 11.7076 })\n",
    "\n",
    "DF = pd.DataFrame(df)\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R Squared (R2) on test data = 0.678511\n",
    "Root Mean Squared Error (RMSE) on test data = 17.932\n",
    "Mean Absolute Error (MAE) on test data = 9.42207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "\n",
    "R Squared (R2) on test data = 0.739797\n",
    "Root Mean Squared Error (RMSE) on test data = 16.9819\n",
    "Mean Absolute Error (MAE) on test data = 8.91377\n",
    "\n",
    "\n",
    "Grid:\n",
    "R Squared (R2) on test data = 0.700806\n",
    "Root Mean Squared Error (RMSE) on test data = 18.2102\n",
    "Mean Absolute Error (MAE) on test data = 9.28254\n",
    "\n",
    "Grid +StandScale\n",
    "R Squared (R2) on test data = 0.700806\n",
    "Root Mean Squared Error (RMSE) on test data = 18.2102\n",
    "Mean Absolute Error (MAE) on test data = 9.28254"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
